{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a classification model to predict those who will likely accept the offer of a new personal loan , by analyzing the previous historical campaign's customer behaviour data.\n",
    "\n",
    "- Age\tCustomer's age in completed years\t\t\t\t\t\t\t\n",
    "- Experience\t#years of professional experience\t\t\t\t\t\t\t\n",
    "- Income\tAnnual income of the customer \t\t\t\t\t\t\n",
    "- Family\tFamily size of the customer\t\t\t\t\t\n",
    "- CCAvg\tAvg. spending on credit cards per month \t\t\t\t\t\t\n",
    "- Education\tEducation Level. 1: Undergrad; 2: Graduate; 3: Advanced/Professional\t\t\t\t\t\t\t\n",
    "- Mortgage\tValue of house mortgage if any. \t\t\t\t\t\t\t\n",
    "- Securities Account\tDoes the customer have a securities account with the bank?\t\t\t\t\t\t\t\n",
    "- CD Account\tDoes the customer have a certificate of deposit (CD) account with the bank?\t\t\t\t\t\t\t\n",
    "- Online\tDoes the customer use internet banking facilities\t\t\t\t\t\t\t\n",
    "- CreditCard\tDoes the customer use a credit card issued by Bank?\t\t\t\t\t\t\t\n",
    "- Offer_acceptance\tDid this customer accept the personal loan offered in the last campaign?\t\t\t\t\t\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_excel('marketing_campaign.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features x and resopnse y\n",
    "x = input_data.drop(['Offer_acceptance'], axis=1)\n",
    "y = input_data['Offer_acceptance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Models using Scikit Learn\n",
    "\n",
    "- Classification classification Model\n",
    "- steps in sckit-learn for model training and prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit 6 steps process to build the predictibe model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree # we are gong to use Tree Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctree)  # all the default parameters got set, we can speficify our own differrent parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ctree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame()\n",
    "comp_df['Class_actual'] = y_test\n",
    "comp_df['Class_predicted'] = y_pred\n",
    "#comp_df[comp_df.Class_actual != comp_df.Class_predicted]\n",
    "comp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification Accuracy = 81.7% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "ctree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imp = pd.DataFrame()\n",
    "temp_imp['col'] = x.columns\n",
    "temp_imp['imp'] = ctree.feature_importances_*100\n",
    "\n",
    "temp_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_imp.sort_values('imp', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NULL Accuracy :\n",
    "- Predict the most frequent class and calcualte the Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # max_depth (default = None): Maximum Depth of the Tree\n",
    "ctree = tree.DecisionTreeClassifier() # 2, 3, 4\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.externals.six import StringIO\n",
    "# import pydotplus\n",
    "\n",
    "# import graphviz\n",
    "# tree_data = tree.export_graphviz(ctree, out_file=None, \n",
    "#                                  feature_names=x.columns, \n",
    "#                                  class_names = ['LA', 'N-LA'],\n",
    "#                                  filled=True,\n",
    "#                                  rounded=True, \n",
    "#                                  special_characters=True\n",
    "#                                  )\n",
    "# graph = graphviz.Source(tree_data)\n",
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree SKLearn Default Model\n",
    "from sklearn import tree\n",
    "ctree = tree.DecisionTreeClassifier()\n",
    "\n",
    "# define features x and resopnse y\n",
    "x = input_data.drop(['Offer_acceptance'], axis=1)\n",
    "y = input_data['Offer_acceptance']\n",
    "\n",
    "# split train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3 )\n",
    "\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred) # Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree = tree.DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "ctree.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = ctree.predict(x_train)\n",
    "print('train_accuracy: ', metrics.accuracy_score(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = ctree.predict(x_test)\n",
    "print('test_accuracy :', metrics.accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth (default = None): Maximum Depth of the Tree\n",
    "ctree = tree.DecisionTreeClassifier(2) # 2, 3, 4\n",
    "ctree = tree.DecisionTreeClassifier(max_depth=4) # 2, 3, 4\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_split (default = None): \n",
    "ctree = tree.DecisionTreeClassifier(min_samples_split=4) # 2, 3, 4\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter Tuning ('Random' or 'best')\n",
    "ctree = tree.DecisionTreeClassifier(splitter ='best') # 2, 3, 4\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight (default = None) , {class_label: weight} - {0: 1, 1: 1}\n",
    "#ctree = tree.DecisionTreeClassifier(class_weight=None) # 2, 3, 4\n",
    "ctree = tree.DecisionTreeClassifier(class_weight={0: 20, 1: 1}) # 2, 3, 4\n",
    "ctree.fit(x_train, y_train)\n",
    "y_pred = ctree.predict(x_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(metrics.recall_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Tuning to improve the model predictive performance on test data\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Ref for Decision Tree Classifier\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \"criterion\"      : ['gini', 'entropy'],    \n",
    "               \"max_features\"   : [2, 3, 4, 5, 6, 7 ],             \n",
    "               \"splitter\"       : ['best', 'random'],  \n",
    "               \"min_samples_split\"  : [3, 5, 10 ]\n",
    "              \n",
    "#              \"class_weight\"      : [{0:1, 1:1}, {0:1, 1:4}]\n",
    "              \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree_model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scoring = {'Accuracy' : 'accuracy', 'Recall' :'recall', 'Precision':'precision', 'AUC':'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = 'accuracy', cv = 5, verbose=3)\n",
    "#optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = 'recall', cv = 5)\n",
    "#optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = 'roc_auc', cv = 5)\n",
    "#optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = 'precision', cv = 5)\n",
    "#optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = scoring, cv = 5, refit='AUC', verbose=1)\n",
    "#optimized_tree = GridSearchCV(ctree_model, param_grid, scoring = scoring, cv = 5, refit='Recall', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_tree.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree_final = optimized_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ctree_final.predict(x_test)\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred) # test accuracy estimate on out of sample data.\n",
    "metrics.recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctree_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data = pd.read_excel('marketing_campaign.xlsx', sheet_name=2)\n",
    "prod_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prod_pred = ctree_final.predict(prod_data)\n",
    "y_prod_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data['y_prod_pred'] = y_prod_pred\n",
    "prod_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Confusion Metrics : describe the performance of the Model\n",
    "\n",
    "- Classification accuracy is the easiest way to understand the classification accuracy\n",
    "- But it does not tell the complete details about classification erros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix( y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic terminology\n",
    "    - True Positives (TP): correctly predicted that they do have diabetes\n",
    "    - True Negatives (TN): correctly predicted that they don't have diabetes\n",
    "    - False Positives (FP): incorrectly predicted that they do have diabetes (a \"Type I error\")\n",
    "    - False Negatives (FN): incorrectly predicted that they don't have diabetes (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix \n",
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TP = confusion[1, 1]\n",
    "\n",
    "print(\"TN - \", TN)\n",
    "print(\"FP - \", FP)\n",
    "print(\"FN - \", FN)\n",
    "print(\"TP - \", TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Confusion matrix give us the complete picture how Classifier is performing\n",
    "- Which Metrics should be used will depend on the business objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Accuracy : \n",
    "\n",
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity(True positive Rate) : when the actual value is positive, how often the prediction correct \n",
    "# Sensitivity = TP / (TP + FN)\n",
    "\n",
    "print(TP / (TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision: When a positive value is predicted, how often is the prediction correct\n",
    "#    How precise is the classifier when predicting positive instances\n",
    "print(TP / (TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity( 1 - FPR)  : when the actual value is negative, how often the prediction correct \n",
    "print( TN/ (TN + FP) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positive Rate : when actual value is negative, how often the prediction incorrect\n",
    "print(FP/(FP + TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Negative Rate : when actual value is Positive, how often the prediction incorrect\n",
    "print(FN/(FN + TP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm Varaince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 50\n",
    "perf = []\n",
    "models = []\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    models.append(ctree_final.fit(x_train, y_train))\n",
    "    y_pred = ctree_final.predict(x_test)\n",
    "    perf.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    \n",
    "perf = np.array(perf)\n",
    "print(perf)\n",
    "print('Avg Performance :', perf.mean())\n",
    "print('Model Variance  :' , perf.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_perf = np.zeros_like(perf, dtype=float)\n",
    "avg_perf.fill(perf.mean())\n",
    "print(avg_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=(16,8) \n",
    "plt.plot(perf, linewidth=3)\n",
    "plt.plot(avg_perf, 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predicted Probablities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?metrics.recall_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob = ctree_final.predict_proba(x_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df=pd.DataFrame()\n",
    "comp_df['prob_1'] = y_pred_prob[:,1]\n",
    "comp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction class based on default threshold prob = 0.5\n",
    "df_prediction = pd.DataFrame()\n",
    "df_prediction['actual_class'] = y_test\n",
    "df_prediction['pred_class'] = y_pred\n",
    "df_prediction['prob_1'] = y_pred_prob[:,1]\n",
    "df_prediction.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Model Performance\n",
    "print( 'accuracy = ' , metrics.accuracy_score(df_prediction.actual_class, df_prediction.pred_class))\n",
    "print( 'sensitivity(tpr) = ' , metrics.recall_score(df_prediction.actual_class, df_prediction.pred_class))\n",
    "print( 'precision = ' , metrics.precision_score(df_prediction.actual_class, df_prediction.pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performane by using the predicted probability at threshold=.45\n",
    "\n",
    "df_prediction.loc[df_prediction['prob_1'] >=.45, 'pred_class_new'] = '1'\n",
    "df_prediction.loc[df_prediction['prob_1'] <.45, 'pred_class_new'] = '0'\n",
    "df_prediction['pred_class_new'] = df_prediction['pred_class_new'].astype(int)\n",
    "\n",
    "#df_prediction.head()\n",
    "print( 'accuracy = ' , metrics.accuracy_score(df_prediction.actual_class, df_prediction.pred_class_new))\n",
    "print( 'sensitivity(tpr) = ' , metrics.recall_score(df_prediction.actual_class, df_prediction.pred_class_new))\n",
    "print( 'precision = ' , metrics.precision_score(df_prediction.actual_class, df_prediction.pred_class_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curver and Area under Curve (AUC) : \n",
    "- Provide the sensitivity and specificity values for differrent threshold values without actually changing the threshold.\n",
    "- ROC will help to choosing the threshold that balance the sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.roc_auc_score(y_test, y_pred)) # AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob[:,1])\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.title('ROC curve for classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC is the percentage of the ROC plot that is underneath the curve:\n",
    "- AUC is useful as a single number summary of classifier performance.\n",
    "- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will   \n",
    "   assign a higher predicted probability to the positive observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = pd.DataFrame()\n",
    "df_threshold['tpr'] = tpr\n",
    "df_threshold['fpr'] = fpr\n",
    "df_threshold['prob'] = thresholds  # probability(1)\n",
    "df_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC suggest the best threshold - tpr=.72, fpr=.21, thresold = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction class based on new threshold prob = 0.24\n",
    "df_prediction.loc[df_prediction['prob_1'] >=.24, 'pred_class_new'] = '1'\n",
    "df_prediction.loc[df_prediction['prob_1'] <.24, 'pred_class_new'] = '0'\n",
    "df_prediction['pred_class_new'] = df_prediction['pred_class_new'].astype(int)\n",
    "df_prediction.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy at threshold=.5\n",
    "print( 'accuracy = ' , metrics.accuracy_score(df_prediction.actual_class, df_prediction.pred_class))\n",
    "print( 'sensitivity(tpr) = ' , metrics.recall_score(df_prediction.actual_class, df_prediction.pred_class))\n",
    "print( 'precision = ' , metrics.precision_score(df_prediction.actual_class, df_prediction.pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy at threshold=.25\n",
    "print( 'accuracy = ' , metrics.accuracy_score(df_prediction.actual_class, df_prediction.pred_class_new))\n",
    "print( 'sensitivity(tpr) = ' , metrics.recall_score(df_prediction.actual_class, df_prediction.pred_class_new))\n",
    "print( 'precision = ' , metrics.precision_score(df_prediction.actual_class, df_prediction.pred_class_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
